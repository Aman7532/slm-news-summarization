{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAN-T5-Base LoRA Fine-tuning for News Summarization (Kaggle GPU)\n",
    "\n",
    "This notebook uses LoRA (Low-Rank Adaptation) for parameter-efficient fine-tuning.\n",
    "\n",
    "**LoRA Benefits**:\n",
    "- Trains only ~0.1% of parameters (much faster)\n",
    "- Lower memory usage\n",
    "- Less prone to catastrophic forgetting\n",
    "- Can be easily merged or switched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets rouge-score bert-score numpy tqdm accelerate sentencepiece peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset class for news summarization\"\"\"\n",
    "    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "        \n",
    "        # Tokenize inputs with prompt for T5\n",
    "        prompt = f\"Summarize the following news article: {text}\"\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        targets = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_dailymail(split='test', num_samples=None):\n",
    "    \"\"\"Load CNN/DailyMail dataset\"\"\"\n",
    "    print(f\"Loading CNN/DailyMail {split} dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=split)\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    texts = [item['article'] for item in dataset]\n",
    "    summaries = [item['highlights'] for item in dataset]\n",
    "    print(f\"Loaded {len(texts)} samples\")\n",
    "    return texts, summaries\n",
    "\n",
    "# Load datasets\n",
    "print(\"Loading datasets...\")\n",
    "train_texts, train_summaries = load_cnn_dailymail('train', num_samples=1000)\n",
    "val_texts, val_summaries = load_cnn_dailymail('validation', num_samples=100)\n",
    "test_texts, test_summaries = load_cnn_dailymail('test', num_samples=100)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_texts)}\")\n",
    "print(f\"  Validation: {len(val_texts)}\")\n",
    "print(f\"  Test: {len(test_texts)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model with LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize base model and tokenizer\n",
    "model_name = \"google/flan-t5-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "base_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "# Configure LoRA\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # Rank of LoRA matrices\n",
    "    lora_alpha=32,  # Scaling factor\n",
    "    target_modules=[\"q\", \"v\"],  # Apply LoRA to query and value projections\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM  # Sequence-to-sequence task\n",
    ")\n",
    "\n",
    "# Get PEFT model (LoRA)\n",
    "model = get_peft_model(base_model, lora_config)\n",
    "model = model.to(device)\n",
    "\n",
    "# Print trainable parameters\n",
    "model.print_trainable_parameters()\n",
    "print(f\"\\nModel loaded on {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = NewsSummarizationDataset(train_texts, train_summaries, tokenizer)\n",
    "val_dataset = NewsSummarizationDataset(val_texts, val_summaries, tokenizer)\n",
    "print(\"Datasets created!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA Fine-tuning Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "output_dir = \"./flan_t5_base_lora_finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Can use larger batch size with LoRA\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=100,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{output_dir}/logs',\n",
    "    logging_steps=50,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "print(\"LoRA Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  LoRA rank: {lora_config.r}\")\n",
    "print(f\"  Mixed precision (FP16): {training_args.fp16}\")\n",
    "print(f\"  Output directory: {output_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start LoRA Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting LoRA fine-tuning...\")\n",
    "print(\"This will be much faster than full fine-tuning!\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nLoRA fine-tuning completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the LoRA adapters\n",
    "print(\"Saving LoRA adapters...\")\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"LoRA adapters saved to {output_dir}\")\n",
    "\n",
    "# Also save to Kaggle output for download\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    kaggle_output_dir = \"/kaggle/working/flan_t5_base_lora_finetuned\"\n",
    "    model.save_pretrained(kaggle_output_dir)\n",
    "    tokenizer.save_pretrained(kaggle_output_dir)\n",
    "    print(f\"LoRA adapters also saved to {kaggle_output_dir} (downloadable from Kaggle)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge LoRA and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge LoRA weights into base model for evaluation\n",
    "print(\"Merging LoRA adapters into base model...\")\n",
    "model = model.merge_and_unload()\n",
    "print(\"Merge complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary(model, tokenizer, text, max_length=128, min_length=30):\n",
    "    \"\"\"Generate summary for a single text\"\"\"\n",
    "    prompt = f\"Summarize the following news article: {text}\"\n",
    "    inputs = tokenizer(\n",
    "        prompt,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "# Generate summaries for test set\n",
    "print(\"Generating summaries on test set...\")\n",
    "generated_summaries = []\n",
    "reference_summaries = test_summaries[:10]  # Evaluate on 10 samples\n",
    "test_texts_subset = test_texts[:10]\n",
    "\n",
    "for text in tqdm(test_texts_subset):\n",
    "    summary = generate_summary(model, tokenizer, text)\n",
    "    generated_summaries.append(summary)\n",
    "\n",
    "print(f\"Generated {len(generated_summaries)} summaries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE scores...\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "for gen_sum, ref_sum in zip(generated_summaries, reference_summaries):\n",
    "    scores = scorer.score(ref_sum, gen_sum)\n",
    "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "# Calculate BERTScore\n",
    "print(\"Calculating BERTScore...\")\n",
    "P, R, F1 = bert_score(generated_summaries, reference_summaries, lang='en', verbose=True)\n",
    "\n",
    "# Compile results\n",
    "results = {\n",
    "    'rouge1': {'f1': np.mean(rouge_scores['rouge1'])},\n",
    "    'rouge2': {'f1': np.mean(rouge_scores['rouge2'])},\n",
    "    'rougeL': {'f1': np.mean(rouge_scores['rougeL'])},\n",
    "    'bertscore': {\n",
    "        'precision': P.mean().item(),\n",
    "        'recall': R.mean().item(),\n",
    "        'f1': F1.mean().item()\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n=== LoRA Fine-tuned Results ===\")\n",
    "print(json.dumps(results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to JSON file\n",
    "results_data = {\n",
    "    \"model\": \"FLAN-T5-Base\",\n",
    "    \"method\": \"LoRA Fine-tuned\",\n",
    "    \"lora_config\": {\n",
    "        \"r\": lora_config.r,\n",
    "        \"lora_alpha\": lora_config.lora_alpha,\n",
    "        \"target_modules\": lora_config.target_modules\n",
    "    },\n",
    "    \"results\": results\n",
    "}\n",
    "\n",
    "with open('flan_t5_base_lora_results.json', 'w') as f:\n",
    "    json.dump(results_data, f, indent=2)\n",
    "print(\"\\nResults saved to flan_t5_base_lora_results.json\")\n",
    "\n",
    "# Save to Kaggle output\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    with open('/kaggle/working/flan_t5_base_lora_results.json', 'w') as f:\n",
    "        json.dump(results_data, f, indent=2)\n",
    "    print(\"Results saved to /kaggle/working/flan_t5_base_lora_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Example Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display example summaries\n",
    "print(\"\\n=== EXAMPLE SUMMARIES ===\\n\")\n",
    "for i in range(min(3, len(test_texts_subset))):\n",
    "    print(f\"--- Example {i+1} ---\")\n",
    "    print(f\"\\nOriginal Article (first 200 chars):\\n{test_texts_subset[i][:200]}...\")\n",
    "    print(f\"\\nReference Summary:\\n{reference_summaries[i]}\")\n",
    "    print(f\"\\nLoRA Fine-tuned Summary:\\n{generated_summaries[i]}\")\n",
    "    print(\"-\" * 80)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}