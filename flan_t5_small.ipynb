{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAN-T5-Small Model for News Summarization\n",
    "\n",
    "This notebook implements Zero-shot, Few-shot, and Fine-tuning approaches for news summarization using FLAN-T5-Small.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets rouge-score bert-score numpy tqdm accelerate sentencepiece\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import json\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset class for news summarization\"\"\"\n",
    "    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        targets = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FLANT5SmallSummarizer:\n",
    "    def __init__(self, model_name=\"google/flan-t5-small\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "    \n",
    "    def zero_shot_summarize(self, text, max_length=128, min_length=30):\n",
    "        \"\"\"Zero-shot summarization using prompt-based approach\"\"\"\n",
    "        prompt = f\"Summarize the following news article: {text}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "    def few_shot_summarize(self, text, examples, max_length=128, min_length=30):\n",
    "        \"\"\"Few-shot summarization with example demonstrations\"\"\"\n",
    "        prompt = \"Summarize the following news articles:\\n\\n\"\n",
    "        \n",
    "        for i, (article, summary) in enumerate(examples, 1):\n",
    "            prompt += f\"Example {i}:\\n\"\n",
    "            prompt += f\"Article: {article[:200]}...\\n\"\n",
    "            prompt += f\"Summary: {summary}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"Now summarize this article:\\n{text}\\nSummary:\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "    def fine_tune(self, train_texts, train_summaries, val_texts=None, val_summaries=None,\n",
    "                  output_dir=\"./flan_t5_small_finetuned\", num_epochs=3, batch_size=8):\n",
    "        \"\"\"Fine-tune the model on news summarization task\"\"\"\n",
    "        train_dataset = NewsSummarizationDataset(train_texts, train_summaries, self.tokenizer)\n",
    "        \n",
    "        val_dataset = None\n",
    "        if val_texts and val_summaries:\n",
    "            val_dataset = NewsSummarizationDataset(val_texts, val_summaries, self.tokenizer)\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=output_dir,\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            warmup_steps=500,\n",
    "            weight_decay=0.01,\n",
    "            logging_dir=f'{output_dir}/logs',\n",
    "            logging_steps=100,\n",
    "            eval_strategy=\"epoch\" if val_dataset else \"no\",\n",
    "            save_strategy=\"epoch\",\n",
    "            load_best_model_at_end=True if val_dataset else False,\n",
    "            save_total_limit=2,\n",
    "        )\n",
    "        \n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "        )\n",
    "        \n",
    "        print(\"Starting fine-tuning...\")\n",
    "        trainer.train()\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "        \n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(output_dir).to(self.device)\n",
    "        print(\"Fine-tuned model loaded\")\n",
    "    \n",
    "    def evaluate(self, texts, reference_summaries, method='zero_shot', examples=None):\n",
    "        \"\"\"Evaluate model performance using ROUGE and BERTScore\"\"\"\n",
    "        generated_summaries = []\n",
    "        \n",
    "        print(f\"Generating summaries using {method}...\")\n",
    "        for text in tqdm(texts):\n",
    "            if method == 'zero_shot':\n",
    "                summary = self.zero_shot_summarize(text)\n",
    "            elif method == 'few_shot' and examples:\n",
    "                summary = self.few_shot_summarize(text, examples)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid method: {method}\")\n",
    "            generated_summaries.append(summary)\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        \n",
    "        for gen_sum, ref_sum in zip(generated_summaries, reference_summaries):\n",
    "            scores = scorer.score(ref_sum, gen_sum)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        print(\"Calculating BERTScore...\")\n",
    "        P, R, F1 = bert_score(generated_summaries, reference_summaries, lang='en', verbose=True)\n",
    "        \n",
    "        results = {\n",
    "            'rouge1': {'f1': np.mean(rouge_scores['rouge1'])},\n",
    "            'rouge2': {'f1': np.mean(rouge_scores['rouge2'])},\n",
    "            'rougeL': {'f1': np.mean(rouge_scores['rougeL'])},\n",
    "            'bertscore': {\n",
    "                'precision': P.mean().item(),\n",
    "                'recall': R.mean().item(),\n",
    "                'f1': F1.mean().item()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results, generated_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_dailymail(split='test', num_samples=100):\n",
    "    \"\"\"Load CNN/DailyMail dataset\"\"\"\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=split)\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    texts = [item['article'] for item in dataset]\n",
    "    summaries = [item['highlights'] for item in dataset]\n",
    "    return texts, summaries\n",
    "\n",
    "def load_xsum(split='test', num_samples=100):\n",
    "    \"\"\"Load XSum dataset\"\"\"\n",
    "    dataset = load_dataset(\"xsum\", split=split)\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    texts = [item['document'] for item in dataset]\n",
    "    summaries = [item['summary'] for item in dataset]\n",
    "    return texts, summaries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd1b4da5a6d74e528461b520d7562fc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23be9932a80d4325b5db6acb962002d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e82f2db04c245b88d3ad3359ee29272",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de1d41074fda4e9f9f8e53de69196cea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aadcab2dbbe46a9b46317c54a83425a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "238e6563b3b7413b8a84a9e6a91f7686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/308M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fbc0acb1d854f79811ed1709179323c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "summarizer = FLANT5SmallSummarizer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CNN/DailyMail dataset...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8ef00d233564dc290e0d4d0a22ad3ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b3c91745294613a8da6b45db0c6402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/train-00000-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a470154dd5a343c28337922084c7fca0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/train-00001-of-00003.parquet:   0%|          | 0.00/257M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9754d594918f44c1acc3796cff3583a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/train-00002-of-00003.parquet:   0%|          | 0.00/259M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2b21a5e58b441b683560e73ab75a4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/validation-00000-of-00001.parquet:   0%|          | 0.00/34.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371493b911d3435382de5d224309487c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "3.0.0/test-00000-of-00001.parquet:   0%|          | 0.00/30.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c8922c9698740148dbdec6ae43ac7f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/287113 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efbccc51f690473bb05d81f19fa7298b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/13368 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59dc5ec62ae14b67b6553ef7017d6b20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/11490 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1000 training samples and 100 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (using smaller subset for demonstration)\n",
    "print(\"Loading CNN/DailyMail dataset...\")\n",
    "train_texts, train_summaries = load_cnn_dailymail('train', num_samples=1000)\n",
    "test_texts, test_summaries = load_cnn_dailymail('test', num_samples=100)\n",
    "print(f\"Loaded {len(train_texts)} training samples and {len(test_texts)} test samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Evaluation ===\n",
      "Generating summaries using zero_shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:10<00:00,  1.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f062c97fc60946459ad590394fdd4b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/25.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a5d705d8994510905548d81d02df5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/482 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4176c9810b8e470082244389a2c10069",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c89ae88608a411cae6de6c65420c076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafbe1bfed8e4f6d98fad5bbf8a33ebe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08aac5ed14db45bb86bf51bdc42dd70c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.42G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdddce58561d4bd3b0fb401b58c2d51d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7eda878530414c9dbea804490e781240",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.84 seconds, 3.52 sentences/sec\n",
      "\n",
      "Zero-shot Results:\n",
      "{\n",
      "  \"rouge1\": {\n",
      "    \"f1\": 0.29555160706390854\n",
      "  },\n",
      "  \"rouge2\": {\n",
      "    \"f1\": 0.11798354330368425\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"f1\": 0.24055417961841913\n",
      "  },\n",
      "  \"bertscore\": {\n",
      "    \"precision\": 0.8781954050064087,\n",
      "    \"recall\": 0.8592444658279419,\n",
      "    \"f1\": 0.8685442805290222\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot evaluation\n",
    "print(\"=== Zero-shot Evaluation ===\")\n",
    "zero_shot_results, zero_shot_summaries = summarizer.evaluate(\n",
    "    test_texts[:10], test_summaries[:10], method='zero_shot'\n",
    ")\n",
    "print(\"\\nZero-shot Results:\")\n",
    "print(json.dumps(zero_shot_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Evaluation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Few-shot Evaluation ===\n",
      "Created 3 few-shot examples from training dataset\n",
      "Generating summaries using few_shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:09<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1945d71f7dab4671a239a3b89bcd91eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8463bea18644858afa5789d9f8347ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.72 seconds, 3.68 sentences/sec\n",
      "\n",
      "Few-shot Results:\n",
      "{\n",
      "  \"rouge1\": {\n",
      "    \"f1\": 0.3228587512150138\n",
      "  },\n",
      "  \"rouge2\": {\n",
      "    \"f1\": 0.12787156474338693\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"f1\": 0.26621045265857635\n",
      "  },\n",
      "  \"bertscore\": {\n",
      "    \"precision\": 0.887690544128418,\n",
      "    \"recall\": 0.8662675619125366,\n",
      "    \"f1\": 0.8767894506454468\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Few-shot evaluation\n",
    "print(\"=== Few-shot Evaluation ===\")\n",
    "# Create few-shot examples dynamically from training dataset\n",
    "few_shot_examples = list(zip(train_texts[:3], train_summaries[:3]))\n",
    "print(f\"Created {len(few_shot_examples)} few-shot examples from training dataset\")\n",
    "\n",
    "few_shot_results, few_shot_summaries = summarizer.evaluate(\n",
    "    test_texts[:10], test_summaries[:10], method='few_shot', examples=few_shot_examples\n",
    ")\n",
    "print(\"\\nFew-shot Results:\")\n",
    "print(json.dumps(few_shot_results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARISON OF RESULTS ===\n",
      "\n",
      "Zero-shot Performance:\n",
      "  ROUGE-1 F1: 0.2956\n",
      "  ROUGE-2 F1: 0.1180\n",
      "  ROUGE-L F1: 0.2406\n",
      "  BERTScore F1: 0.8685\n",
      "\n",
      "Few-shot Performance:\n",
      "  ROUGE-1 F1: 0.3229\n",
      "  ROUGE-2 F1: 0.1279\n",
      "  ROUGE-L F1: 0.2662\n",
      "  BERTScore F1: 0.8768\n",
      "\n",
      "Results saved to flan_t5_small_results.json\n"
     ]
    }
   ],
   "source": [
    "# Compare results from Zero-shot and Few-shot\n",
    "print(\"\\n=== COMPARISON OF RESULTS ===\")\n",
    "print(\"\\nZero-shot Performance:\")\n",
    "print(f\"  ROUGE-1 F1: {zero_shot_results['rouge1']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-2 F1: {zero_shot_results['rouge2']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-L F1: {zero_shot_results['rougeL']['f1']:.4f}\")\n",
    "print(f\"  BERTScore F1: {zero_shot_results['bertscore']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nFew-shot Performance:\")\n",
    "print(f\"  ROUGE-1 F1: {few_shot_results['rouge1']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-2 F1: {few_shot_results['rouge2']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-L F1: {few_shot_results['rougeL']['f1']:.4f}\")\n",
    "print(f\"  BERTScore F1: {few_shot_results['bertscore']['f1']:.4f}\")\n",
    "\n",
    "# Save results to file\n",
    "results_summary = {\n",
    "    \"model\": \"FLAN-T5-Small\",\n",
    "    \"zero_shot\": zero_shot_results,\n",
    "    \"few_shot\": few_shot_results\n",
    "}\n",
    "\n",
    "with open('flan_t5_small_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(\"\\nResults saved to flan_t5_small_results.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE SUMMARIES ===\n",
      "\n",
      "--- Example 1 ---\n",
      "\n",
      "Original Article (first 300 chars):\n",
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the cou...\n",
      "\n",
      "Reference Summary:\n",
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "\n",
      "Zero-shot Summary:\n",
      "The Palestinian Authority has officially become the 123rd member of the International Criminal Court, paving the way for possible war crimes investigations against Israelis.\n",
      "\n",
      "Few-shot Summary:\n",
      "The Palestinian Authority officially became the 123rd member of the International Criminal Court. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "\n",
      "Original Article (first 300 chars):\n",
      "(CNN)Never mind cats having nine lives. A stray pooch in Washington State has used up at least three of her own after being hit by a car, apparently whacked on the head with a hammer in a misguided mercy killing and then buried in a field -- only to survive. That's according to Washington State Univ...\n",
      "\n",
      "Reference Summary:\n",
      "Theia, a bully breed mix, was apparently hit by a car, whacked with a hammer and buried in a field .\n",
      "\"She's a true miracle dog and she deserves a good life,\" says Sara Mellado, who is looking for a home for Theia .\n",
      "\n",
      "Zero-shot Summary:\n",
      "Theia, a stray pooch, has been buried in a field in Washington State for the first time since she was killed in a car accident.\n",
      "\n",
      "Few-shot Summary:\n",
      "A stray pooch in Washington State has used up at least three of her own after being hit by a car and then buried in a field.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "\n",
      "Original Article (first 300 chars):\n",
      "(CNN)If you've been following the news lately, there are certain things you doubtless know about Mohammad Javad Zarif. He is, of course, the Iranian foreign minister. He has been U.S. Secretary of State John Kerry's opposite number in securing a breakthrough in nuclear discussions that could lead to...\n",
      "\n",
      "Reference Summary:\n",
      "Mohammad Javad Zarif has spent more time with John Kerry than any other foreign minister .\n",
      "He once participated in a takeover of the Iranian Consulate in San Francisco .\n",
      "The Iranian foreign minister tweets in English .\n",
      "\n",
      "Zero-shot Summary:\n",
      "U.S. for a year and a half before he was nominated to be foreign minister. He was nominated to be foreign minister by Hassan Rouhami.\n",
      "\n",
      "Few-shot Summary:\n",
      "Mohammad Javad Zarif arrived in Iran on Friday morning, a day after a visit to the United States to discuss the nuclear crisis.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display example summaries for comparison\n",
    "print(\"\\n=== EXAMPLE SUMMARIES ===\")\n",
    "for i in range(min(3, len(test_texts))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"\\nOriginal Article (first 300 chars):\\n{test_texts[i][:300]}...\")\n",
    "    print(f\"\\nReference Summary:\\n{test_summaries[i]}\")\n",
    "    print(f\"\\nZero-shot Summary:\\n{zero_shot_summaries[i]}\")\n",
    "    print(f\"\\nFew-shot Summary:\\n{few_shot_summaries[i]}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
