{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BART-Base Fine-tuning for News Summarization (Kaggle GPU)\n",
    "\n",
    "This notebook is designed to run on Kaggle with GPU acceleration for fine-tuning BART-Base model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets rouge-score bert-score numpy tqdm accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "# Check GPU availability\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class\n",
    "\n",
    "This dataset class handles tokenization for BART models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Class Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset class for news summarization\"\"\"\n",
    "    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "        \n",
    "        # Tokenize inputs with prompt\n",
    "        # BART doesn't need prompts, use text directly\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        targets = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_dailymail(split='test', num_samples=None):\n",
    "    \"\"\"Load CNN/DailyMail dataset\"\"\"\n",
    "    print(f\"Loading CNN/DailyMail {split} dataset...\")\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=split)\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    texts = [item['article'] for item in dataset]\n",
    "    summaries = [item['highlights'] for item in dataset]\n",
    "    print(f\"Loaded {len(texts)} samples\")\n",
    "    return texts, summaries\n",
    "\n",
    "# Load datasets - adjust num_samples based on your GPU memory\n",
    "# For Kaggle GPU (16GB), you can use more samples\n",
    "print(\"Loading datasets...\")\n",
    "train_texts, train_summaries = load_cnn_dailymail('train', num_samples=5000)\n",
    "val_texts, val_summaries = load_cnn_dailymail('validation', num_samples=500)\n",
    "test_texts, test_summaries = load_cnn_dailymail('test', num_samples=200)\n",
    "\n",
    "print(f\"\\nDataset sizes:\")\n",
    "print(f\"  Training: {len(train_texts)}\")\n",
    "print(f\"  Validation: {len(val_texts)}\")\n",
    "print(f\"  Test: {len(test_texts)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model and tokenizer\n",
    "model_name = \"facebook/bart-base\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Loading {model_name}...\")\n",
    "tokenizer = BartTokenizer.from_pretrained(model_name)\n",
    "model = BartForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "print(f\"Model loaded on {device}\")\n",
    "\n",
    "# Print model size\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "print(\"Creating datasets...\")\n",
    "train_dataset = NewsSummarizationDataset(train_texts, train_summaries, tokenizer)\n",
    "val_dataset = NewsSummarizationDataset(val_texts, val_summaries, tokenizer)\n",
    "print(\"Datasets created!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training arguments\n",
    "output_dir = \"./bart_base_finetuned\"\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,  # Adjust based on GPU memory\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=f'{output_dir}/logs',\n",
    "    logging_steps=100,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    save_total_limit=2,\n",
    "    prediction_loss_only=True,\n",
    "    fp16=torch.cuda.is_available(),  # Use mixed precision if GPU available\n",
    "    report_to=\"none\",  # Disable wandb/tensorboard for Kaggle\n",
    ")\n",
    "\n",
    "print(\"Training configuration:\")\n",
    "print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"  Mixed precision (FP16): {training_args.fp16}\")\n",
    "print(f\"  Output directory: {output_dir}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Fine-tuning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    ")\n",
    "\n",
    "print(\"Starting fine-tuning...\")\n",
    "print(\"This may take several hours depending on dataset size and GPU.\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Train\n",
    "trainer.train()\n",
    "\n",
    "print(\"\\nFine-tuning completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the fine-tuned model\n",
    "print(\"Saving model...\")\n",
    "trainer.save_model(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)\n",
    "print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "# Also save to Kaggle output for download\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    kaggle_output_dir = \"/kaggle/working/bart_base_finetuned\"\n",
    "    trainer.save_model(kaggle_output_dir)\n",
    "    tokenizer.save_pretrained(kaggle_output_dir)\n",
    "    print(f\"Model also saved to {kaggle_output_dir} (downloadable from Kaggle)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation on Test Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the best model\n",
    "print(\"Loading best checkpoint...\")\n",
    "model = BartForConditionalGeneration.from_pretrained(output_dir).to(device)\n",
    "print(\"Best model loaded!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate summaries for test set\n",
    "def generate_summary(text, max_length=128, min_length=30):\n",
    "    \"\"\"Generate summary for a single text\"\"\"\n",
    "        # BART doesn't need prompts, use text directly\n",
    "    inputs = tokenizer(\n",
    "        text,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_tensors='pt'\n",
    "    ).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            input_ids=inputs['input_ids'],\n",
    "            attention_mask=inputs['attention_mask'],\n",
    "            max_length=max_length,\n",
    "            min_length=min_length,\n",
    "            num_beams=4,\n",
    "            length_penalty=2.0,\n",
    "            early_stopping=True\n",
    "        )\n",
    "    \n",
    "    summary = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return summary\n",
    "\n",
    "print(\"Generating summaries for test set...\")\n",
    "generated_summaries = []\n",
    "for text in tqdm(test_texts):\n",
    "    summary = generate_summary(text)\n",
    "    generated_summaries.append(summary)\n",
    "\n",
    "print(f\"Generated {len(generated_summaries)} summaries\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate ROUGE scores\n",
    "print(\"Calculating ROUGE scores...\")\n",
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "\n",
    "for gen_sum, ref_sum in zip(generated_summaries, test_summaries):\n",
    "    scores = scorer.score(ref_sum, gen_sum)\n",
    "    rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "    rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "    rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "\n",
    "print(\"Calculating BERTScore...\")\n",
    "P, R, F1 = bert_score(generated_summaries, test_summaries, lang='en', verbose=True)\n",
    "\n",
    "results = {\n",
    "    'rouge1': {'f1': float(np.mean(rouge_scores['rouge1']))},\n",
    "    'rouge2': {'f1': float(np.mean(rouge_scores['rouge2']))},\n",
    "    'rougeL': {'f1': float(np.mean(rouge_scores['rougeL']))},\n",
    "    'bertscore': {\n",
    "        'precision': float(P.mean().item()),\n",
    "        'recall': float(R.mean().item()),\n",
    "        'f1': float(F1.mean().item())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n=== FINE-TUNED MODEL RESULTS ===\")\n",
    "print(json.dumps(results, indent=2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "results_summary = {\n",
    "    \"model\": \"BART-Base (Fine-tuned)\",\n",
    "    \"training_samples\": len(train_texts),\n",
    "    \"validation_samples\": len(val_texts),\n",
    "    \"test_samples\": len(test_texts),\n",
    "    \"metrics\": results\n",
    "}\n",
    "\n",
    "with open('bart_base_finetuned_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "# Save to Kaggle output if available\n",
    "if os.path.exists('/kaggle/working'):\n",
    "    with open('/kaggle/working/bart_base_finetuned_results.json', 'w') as f:\n",
    "        json.dump(results_summary, f, indent=2)\n",
    "    print(\"Results saved to /kaggle/working/bart_base_finetuned_results.json\")\n",
    "\n",
    "print(\"\\nResults saved!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display some example summaries\n",
    "print(\"\\n=== EXAMPLE SUMMARIES ===\")\n",
    "for i in range(min(5, len(test_texts))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"\\nOriginal Article (first 300 chars):\\n{test_texts[i][:300]}...\")\n",
    "    print(f\"\\nReference Summary:\\n{test_summaries[i]}\")\n",
    "    print(f\"\\nGenerated Summary:\\n{generated_summaries[i]}\")\n",
    "    print(\"-\" * 80)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}