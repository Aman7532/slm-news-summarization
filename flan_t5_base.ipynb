{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FLAN-T5-Base Model for News Summarization\n",
    "\n",
    "This notebook implements Zero-shot, Few-shot, and Fine-tuning approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "!pip install -q torch transformers datasets rouge-score bert-score numpy tqdm accelerate sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, Trainer, TrainingArguments\n",
    "from datasets import load_dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from rouge_score import rouge_scorer\n",
    "from bert_score import score as bert_score\n",
    "import json\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NewsSummarizationDataset(Dataset):\n",
    "    \"\"\"Dataset class for news summarization\"\"\"\n",
    "    def __init__(self, texts, summaries, tokenizer, max_input_length=512, max_target_length=128):\n",
    "        self.texts = texts\n",
    "        self.summaries = summaries\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_length = max_input_length\n",
    "        self.max_target_length = max_target_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        summary = str(self.summaries[idx])\n",
    "        \n",
    "        # Tokenize inputs\n",
    "        inputs = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_input_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        # Tokenize targets\n",
    "        targets = self.tokenizer(\n",
    "            summary,\n",
    "            max_length=self.max_target_length,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'labels': targets['input_ids'].squeeze()\n",
    "        }\n",
    "\n",
    "\n",
    "class FLANT5BaseSummarizer:\n",
    "    def __init__(self, model_name=\"google/flan-t5-base\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "        self.model = T5ForConditionalGeneration.from_pretrained(model_name).to(self.device)\n",
    "        print(f\"Model loaded on {self.device}\")\n",
    "    \n",
    "    def zero_shot_summarize(self, text, max_length=128, min_length=30):\n",
    "        \"\"\"Zero-shot summarization using prompt-based approach\"\"\"\n",
    "        prompt = f\"Summarize the following news article: {text}\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "    def few_shot_summarize(self, text, examples, max_length=128, min_length=30):\n",
    "        \"\"\"Few-shot summarization with example demonstrations\"\"\"\n",
    "        prompt = \"Summarize the following news articles:\\n\\n\"\n",
    "        \n",
    "        for i, (article, summary) in enumerate(examples, 1):\n",
    "            prompt += f\"Example {i}:\\n\"\n",
    "            prompt += f\"Article: {article[:200]}...\\n\"\n",
    "            prompt += f\"Summary: {summary}\\n\\n\"\n",
    "        \n",
    "        prompt += f\"Now summarize this article:\\n{text}\\nSummary:\"\n",
    "        \n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            max_length=512,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        ).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                input_ids=inputs['input_ids'],\n",
    "                attention_mask=inputs['attention_mask'],\n",
    "                max_length=max_length,\n",
    "                min_length=min_length,\n",
    "                num_beams=4,\n",
    "                length_penalty=2.0,\n",
    "                early_stopping=True\n",
    "            )\n",
    "        \n",
    "        summary = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "    \n",
    "    def evaluate(self, texts, reference_summaries, method='zero_shot', examples=None):\n",
    "        \"\"\"Evaluate model performance using ROUGE and BERTScore\"\"\"\n",
    "        generated_summaries = []\n",
    "        \n",
    "        print(f\"Generating summaries using {method}...\")\n",
    "        for text in tqdm(texts):\n",
    "            if method == 'zero_shot':\n",
    "                summary = self.zero_shot_summarize(text)\n",
    "            elif method == 'few_shot' and examples:\n",
    "                summary = self.few_shot_summarize(text, examples)\n",
    "            else:\n",
    "                raise ValueError(f\"Invalid method: {method}\")\n",
    "            generated_summaries.append(summary)\n",
    "        \n",
    "        scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "        rouge_scores = {'rouge1': [], 'rouge2': [], 'rougeL': []}\n",
    "        \n",
    "        for gen_sum, ref_sum in zip(generated_summaries, reference_summaries):\n",
    "            scores = scorer.score(ref_sum, gen_sum)\n",
    "            rouge_scores['rouge1'].append(scores['rouge1'].fmeasure)\n",
    "            rouge_scores['rouge2'].append(scores['rouge2'].fmeasure)\n",
    "            rouge_scores['rougeL'].append(scores['rougeL'].fmeasure)\n",
    "        \n",
    "        print(\"Calculating BERTScore...\")\n",
    "        P, R, F1 = bert_score(generated_summaries, reference_summaries, lang='en', verbose=True)\n",
    "        \n",
    "        results = {\n",
    "            'rouge1': {'f1': np.mean(rouge_scores['rouge1'])},\n",
    "            'rouge2': {'f1': np.mean(rouge_scores['rouge2'])},\n",
    "            'rougeL': {'f1': np.mean(rouge_scores['rougeL'])},\n",
    "            'bertscore': {\n",
    "                'precision': P.mean().item(),\n",
    "                'recall': R.mean().item(),\n",
    "                'f1': F1.mean().item()\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return results, generated_summaries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cnn_dailymail(split='test', num_samples=100):\n",
    "    \"\"\"Load CNN/DailyMail dataset\"\"\"\n",
    "    dataset = load_dataset(\"cnn_dailymail\", \"3.0.0\", split=split)\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    texts = [item['article'] for item in dataset]\n",
    "    summaries = [item['highlights'] for item in dataset]\n",
    "    return texts, summaries\n",
    "\n",
    "def load_xsum(split='test', num_samples=100):\n",
    "    \"\"\"Load XSum dataset\"\"\"\n",
    "    dataset = load_dataset(\"xsum\", split=split)\n",
    "    if num_samples:\n",
    "        dataset = dataset.select(range(min(num_samples, len(dataset))))\n",
    "    texts = [item['document'] for item in dataset]\n",
    "    summaries = [item['summary'] for item in dataset]\n",
    "    return texts, summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad22e9a8a9940aea8a26f177ce1de04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10f6ae31fddf4fca9ec613bd00fe963c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e8686fb28e342b095286f4aafbb3e66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c167497911554ad8bf04b41aa239ae18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6d073496e4b494cb2914590850f6b64",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1bea74f75474c3783682e997fb645b4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/990M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a50acb1f38434806988371f22f09269e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on cpu\n"
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "summarizer = FLANT5BaseSummarizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CNN/DailyMail dataset...\n",
      "Loaded 1000 training samples and 100 test samples\n"
     ]
    }
   ],
   "source": [
    "# Load dataset (using smaller subset for demonstration)\n",
    "print(\"Loading CNN/DailyMail dataset...\")\n",
    "train_texts, train_summaries = load_cnn_dailymail('train', num_samples=1000)\n",
    "test_texts, test_summaries = load_cnn_dailymail('test', num_samples=100)\n",
    "print(f\"Loaded {len(train_texts)} training samples and {len(test_texts)} test samples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zero-shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Zero-shot Evaluation ===\n",
      "Generating summaries using zero_shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:29<00:00,  2.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4d7ed894704bc19f5a8c63bdc94177",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359ea9a96cb54077820b9dbed3bf1ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.60 seconds, 3.85 sentences/sec\n",
      "\n",
      "Zero-shot Results:\n",
      "{\n",
      "  \"rouge1\": {\n",
      "    \"f1\": 0.32504652398970296\n",
      "  },\n",
      "  \"rouge2\": {\n",
      "    \"f1\": 0.13829923433039357\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"f1\": 0.24898921840528607\n",
      "  },\n",
      "  \"bertscore\": {\n",
      "    \"precision\": 0.8817199468612671,\n",
      "    \"recall\": 0.8624206781387329,\n",
      "    \"f1\": 0.8719387054443359\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Zero-shot evaluation\n",
    "print(\"=== Zero-shot Evaluation ===\")\n",
    "zero_shot_results, zero_shot_summaries = summarizer.evaluate(\n",
    "    test_texts[:10], test_summaries[:10], method='zero_shot'\n",
    ")\n",
    "print(\"\\nZero-shot Results:\")\n",
    "print(json.dumps(zero_shot_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Few-shot Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Few-shot Evaluation ===\n",
      "Created 3 few-shot examples from training dataset\n",
      "Generating summaries using few_shot...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:22<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculating BERTScore...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-large and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calculating scores...\n",
      "computing bert embedding.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "600f87d3dfb14b539e94e84ff39f04b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "computing greedy matching.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "354fa25778374242863e9596804840ff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done in 2.38 seconds, 4.21 sentences/sec\n",
      "\n",
      "Few-shot Results:\n",
      "{\n",
      "  \"rouge1\": {\n",
      "    \"f1\": 0.27626319629513074\n",
      "  },\n",
      "  \"rouge2\": {\n",
      "    \"f1\": 0.10690527984124629\n",
      "  },\n",
      "  \"rougeL\": {\n",
      "    \"f1\": 0.20366161325329096\n",
      "  },\n",
      "  \"bertscore\": {\n",
      "    \"precision\": 0.872004508972168,\n",
      "    \"recall\": 0.848450779914856,\n",
      "    \"f1\": 0.8600193858146667\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Few-shot evaluation\n",
    "print(\"=== Few-shot Evaluation ===\")\n",
    "# Create few-shot examples dynamically from training dataset\n",
    "few_shot_examples = list(zip(train_texts[:3], train_summaries[:3]))\n",
    "print(f\"Created {len(few_shot_examples)} few-shot examples from training dataset\")\n",
    "\n",
    "few_shot_results, few_shot_summaries = summarizer.evaluate(\n",
    "    test_texts[:10], test_summaries[:10], method='few_shot', examples=few_shot_examples\n",
    ")\n",
    "print(\"\\nFew-shot Results:\")\n",
    "print(json.dumps(few_shot_results, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison of Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPARISON OF RESULTS ===\n",
      "\n",
      "Zero-shot Performance:\n",
      "  ROUGE-1 F1: 0.3250\n",
      "  ROUGE-2 F1: 0.1383\n",
      "  ROUGE-L F1: 0.2490\n",
      "  BERTScore F1: 0.8719\n",
      "\n",
      "Few-shot Performance:\n",
      "  ROUGE-1 F1: 0.2763\n",
      "  ROUGE-2 F1: 0.1069\n",
      "  ROUGE-L F1: 0.2037\n",
      "  BERTScore F1: 0.8600\n",
      "\n",
      "Results saved to flan_t5_base_results.json\n"
     ]
    }
   ],
   "source": [
    "# Compare results from Zero-shot and Few-shot\n",
    "print(\"\\n=== COMPARISON OF RESULTS ===\")\n",
    "print(\"\\nZero-shot Performance:\")\n",
    "print(f\"  ROUGE-1 F1: {zero_shot_results['rouge1']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-2 F1: {zero_shot_results['rouge2']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-L F1: {zero_shot_results['rougeL']['f1']:.4f}\")\n",
    "print(f\"  BERTScore F1: {zero_shot_results['bertscore']['f1']:.4f}\")\n",
    "\n",
    "print(\"\\nFew-shot Performance:\")\n",
    "print(f\"  ROUGE-1 F1: {few_shot_results['rouge1']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-2 F1: {few_shot_results['rouge2']['f1']:.4f}\")\n",
    "print(f\"  ROUGE-L F1: {few_shot_results['rougeL']['f1']:.4f}\")\n",
    "print(f\"  BERTScore F1: {few_shot_results['bertscore']['f1']:.4f}\")\n",
    "\n",
    "# Save results to file\n",
    "results_summary = {\n",
    "    \"model\": \"FLAN-T5-Base\",\n",
    "    \"zero_shot\": zero_shot_results,\n",
    "    \"few_shot\": few_shot_results\n",
    "}\n",
    "\n",
    "with open('flan_t5_base_results.json', 'w') as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "print(\"\\nResults saved to flan_t5_base_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== EXAMPLE SUMMARIES ===\n",
      "\n",
      "--- Example 1 ---\n",
      "\n",
      "Original Article (first 300 chars):\n",
      "(CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the cou...\n",
      "\n",
      "Reference Summary:\n",
      "Membership gives the ICC jurisdiction over alleged crimes committed in Palestinian territories since last June .\n",
      "Israel and the United States opposed the move, which could open the door to war crimes investigations against Israelis .\n",
      "\n",
      "Zero-shot Summary:\n",
      "Palestinians should be allowed to join the ICC, a move that could lead to war crimes investigations against Israelis, a court official said.\n",
      "\n",
      "Few-shot Summary:\n",
      "The Palestinian Authority has become the 123rd member of the ICC, a step that gives the court jurisdiction over alleged crimes in Palestinian territories.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Example 2 ---\n",
      "\n",
      "Original Article (first 300 chars):\n",
      "(CNN)Never mind cats having nine lives. A stray pooch in Washington State has used up at least three of her own after being hit by a car, apparently whacked on the head with a hammer in a misguided mercy killing and then buried in a field -- only to survive. That's according to Washington State Univ...\n",
      "\n",
      "Reference Summary:\n",
      "Theia, a bully breed mix, was apparently hit by a car, whacked with a hammer and buried in a field .\n",
      "\"She's a true miracle dog and she deserves a good life,\" says Sara Mellado, who is looking for a home for Theia .\n",
      "\n",
      "Zero-shot Summary:\n",
      "A stray dog in Washington State has used up at least three of her own after being hit by a car in a misguided mercy killing and buried in a field -- only to survive.\n",
      "\n",
      "Few-shot Summary:\n",
      "Washington State University hospital, where she was treated for a broken skull and a broken rib cage, according to a news release from the hospital.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Example 3 ---\n",
      "\n",
      "Original Article (first 300 chars):\n",
      "(CNN)If you've been following the news lately, there are certain things you doubtless know about Mohammad Javad Zarif. He is, of course, the Iranian foreign minister. He has been U.S. Secretary of State John Kerry's opposite number in securing a breakthrough in nuclear discussions that could lead to...\n",
      "\n",
      "Reference Summary:\n",
      "Mohammad Javad Zarif has spent more time with John Kerry than any other foreign minister .\n",
      "He once participated in a takeover of the Iranian Consulate in San Francisco .\n",
      "The Iranian foreign minister tweets in English .\n",
      "\n",
      "Zero-shot Summary:\n",
      "U.S. last year, and he has been a key figure in bringing Iran back to the international community, but he has also been a key figure in bringing Iran into the international community.\n",
      "\n",
      "Few-shot Summary:\n",
      "U.S. ambassador to Iran greeted him with a resounding admonishment, \"It's a long way from here.\"\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Display example summaries for comparison\n",
    "print(\"\\n=== EXAMPLE SUMMARIES ===\")\n",
    "for i in range(min(3, len(test_texts))):\n",
    "    print(f\"\\n--- Example {i+1} ---\")\n",
    "    print(f\"\\nOriginal Article (first 300 chars):\\n{test_texts[i][:300]}...\")\n",
    "    print(f\"\\nReference Summary:\\n{test_summaries[i]}\")\n",
    "    print(f\"\\nZero-shot Summary:\\n{zero_shot_summaries[i]}\")\n",
    "    print(f\"\\nFew-shot Summary:\\n{few_shot_summaries[i]}\")\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
